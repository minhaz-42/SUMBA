# ğŸ¤Ÿ SUMBA â€” 3D Sign Language Translation Platform

<p align="center">
  <img src="docs/logo.png" alt="SUMBA Logo" width="180">
</p>

<p align="center">
  <strong>Capture â€¢ Train â€¢ Translate â€” Real-Time Sign Language to Text</strong>
</p>

<p align="center">
  <a href="#features">Features</a> â€¢
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#architecture">Architecture</a> â€¢
  <a href="#supported-languages">Languages</a> â€¢
  <a href="#api">API</a> â€¢
  <a href="#contributing">Contributing</a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue?logo=python" alt="Python">
  <img src="https://img.shields.io/badge/Django-4.2-green?logo=django" alt="Django">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red?logo=pytorch" alt="PyTorch">
  <img src="https://img.shields.io/badge/License-MIT-yellow" alt="License">
</p>

---

## Overview

SUMBA is a research-grade platform for **3D sign language translation**. Unlike traditional 2D video-based approaches, SUMBA captures **21 hand joint positions in 3D space** (X, Y, Z coordinates) using MediaPipe, enabling more accurate gesture recognition and translation.

### Why 3D Beats 2D

| 2D Video Limitations | 3D Skeletal Advantages |
|---------------------|------------------------|
| âŒ Depth ambiguity | âœ… Full spatial information (X, Y, Z) |
| âŒ Occlusion issues | âœ… View invariance |
| âŒ Lighting dependency | âœ… Works in any lighting |
| âŒ Background noise | âœ… Compact skeletal representation |

---

## âœ¨ Features

- **ğŸ¥ Real-Time Capture** â€” MediaPipe-powered hand tracking at 30fps from any webcam
- **ğŸ¦´ 3D Skeleton Data** â€” 21 hand joints Ã— 3 coordinates per frame
- **ğŸ“¦ Dataset Management** â€” Build labeled datasets with train/validation/test splits
- **ğŸ§  Multiple Models** â€” ST-GCN, Transformer, or Hybrid architectures
- **ğŸ“ˆ Training Dashboard** â€” Monitor training progress with real-time metrics
- **ğŸŒ Multi-Language** â€” Support for 8+ sign languages (ASL, BdSL, BSL, ISL, JSL, CSL, DGS, LSF)
- **ğŸ”Œ REST API** â€” Full API access for integration with external systems
- **âš¡ WebSocket Streaming** â€” Real-time inference for live translation

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- PostgreSQL 14+ (or SQLite for development)
- Redis (optional, for WebSocket support)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/sumba.git
cd sumba/signlang3d

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run migrations
cd backend
python manage.py migrate

# Load initial data (languages, joints, tags, model architectures)
python manage.py setup_initial_data

# Create admin user (optional)
python manage.py createsuperuser

# Start development server
USE_SQLITE=true python manage.py runserver
```

Visit **http://localhost:8000** to access the platform.

### Default Credentials

After running `setup_initial_data`:
- **Username:** `admin`
- **Password:** `admin123`

---

## ğŸ“Š Supported Languages

| Flag | Code | Language |
|------|------|----------|
| ğŸ‡ºğŸ‡¸ | ASL | American Sign Language |
| ğŸ‡§ğŸ‡© | BdSL | Bangladeshi Sign Language |
| ğŸ‡¬ğŸ‡§ | BSL | British Sign Language |
| ğŸ‡®ğŸ‡³ | ISL | Indian Sign Language |
| ğŸ‡¯ğŸ‡µ | JSL | Japanese Sign Language |
| ğŸ‡¨ğŸ‡³ | CSL | Chinese Sign Language |
| ğŸ‡©ğŸ‡ª | DGS | German Sign Language |
| ğŸ‡«ğŸ‡· | LSF | French Sign Language |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Webcam        â”‚â”€â”€â”€â–¶â”‚  MediaPipe      â”‚â”€â”€â”€â–¶â”‚  ST-GCN/Trans   â”‚â”€â”€â”€â–¶â”‚  Text Decoder   â”‚
â”‚   (30 fps)      â”‚    â”‚  (21 jointsÃ—3D) â”‚    â”‚  (Encoder)      â”‚    â”‚  (Transformer)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚                      â”‚
        â–¼                      â–¼                      â–¼                      â–¼
   Camera input          Joint positions        Motion embeddings       Text output
                        (x, y, z) Ã— T          (B, T', D)             "Hello"
```

### Model Architectures

| Model | Description | Best For |
|-------|-------------|----------|
| **ST-GCN** | Spatial-Temporal Graph ConvNet | Fast inference, smaller datasets |
| **Transformer** | Self-attention encoder | Large datasets, complex gestures |
| **Hybrid** | ST-GCN + Transformer | Best accuracy, research use |

---

## ğŸ“ Project Structure

```
signlang3d/
â”œâ”€â”€ backend/                    # Django Backend
â”‚   â”œâ”€â”€ core/                   # Project settings & routing
â”‚   â”œâ”€â”€ accounts/               # User management & profiles
â”‚   â”œâ”€â”€ gestures/               # Gesture samples & WebSocket
â”‚   â”œâ”€â”€ datasets/               # Dataset versioning & splits
â”‚   â”œâ”€â”€ training/               # Training runs & checkpoints
â”‚   â”œâ”€â”€ inference/              # Inference & model deployment
â”‚   â””â”€â”€ api/                    # REST API (DRF)
â”œâ”€â”€ ml/                         # PyTorch ML Code
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ stgcn.py           # ST-GCN implementation
â”‚   â”‚   â”œâ”€â”€ motion_transformer.py
â”‚   â”‚   â””â”€â”€ decoder.py         # Text decoder
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ sign_language.py   # PyTorch Dataset
â”‚   â”œâ”€â”€ train.py               # Training pipeline
â”‚   â””â”€â”€ infer.py               # Inference pipeline
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ templates/             # Django templates (Tailwind CSS)
â”œâ”€â”€ checkpoints/               # Saved model weights
â”œâ”€â”€ media/                     # Uploaded files
â””â”€â”€ requirements.txt
```

---

## ğŸ”Œ API Reference

### REST Endpoints

```bash
# List gesture samples
GET /api/gestures/

# Create gesture sample
POST /api/gestures/
{
    "language": "ASL",
    "gloss": "hello",
    "frames": [{"joints": [[x,y,z], ...], "timestamp": 0}, ...]
}

# Run inference
POST /api/inference/
{
    "model": "hybrid",
    "language": "ASL",
    "frames": [...]
}
```

### WebSocket Endpoints

```javascript
// Connect to gesture capture
const ws = new WebSocket('ws://localhost:8000/ws/gesture/capture/');

// Send frame data
ws.send(JSON.stringify({ 
    type: 'frame', 
    data: { timestamp: 123, joints: [...] } 
}));

// Receive response
ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log('Translation:', data.translation);
};
```

---

## ğŸ‹ï¸ Training

### Run Training

```bash
python ml/train.py \
    --data_dir data/ASL \
    --model_type hybrid \
    --batch_size 32 \
    --epochs 50 \
    --learning_rate 1e-4 \
    --output_dir checkpoints/hybrid_asl_v1
```

### Training Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--model_type` | hybrid | Architecture (stgcn, transformer, hybrid) |
| `--batch_size` | 32 | Training batch size |
| `--epochs` | 50 | Number of epochs |
| `--learning_rate` | 1e-4 | Initial learning rate |
| `--warmup_steps` | 1000 | LR warmup steps |
| `--gradient_clip` | 1.0 | Gradient clipping |
| `--label_smoothing` | 0.1 | Label smoothing factor |

### Evaluation Metrics

| Metric | Description |
|--------|-------------|
| **BLEU-4** | Translation quality (higher is better) |
| **WER** | Word Error Rate (lower is better) |
| **CER** | Character Error Rate (lower is better) |

---

## ğŸ³ Docker

```bash
# Build and run with Docker Compose
docker-compose up --build

# Access at http://localhost:8000
```

---

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ“š Citation

If you use SUMBA in your research, please cite:

```bibtex
@software{sumba2026,
    title={SUMBA: 3D Sign Language Translation Platform},
    author={Your Name},
    year={2026},
    url={https://github.com/yourusername/sumba}
}
```

---

## ğŸ™ Acknowledgments

- [MediaPipe](https://mediapipe.dev/) â€” Hand tracking
- [PyTorch](https://pytorch.org/) â€” Deep learning framework
- [Django](https://www.djangoproject.com/) â€” Web framework
- [Tailwind CSS](https://tailwindcss.com/) â€” Styling

---

<p align="center">
  Made with â¤ï¸ for the Deaf and Hard of Hearing community
</p>
